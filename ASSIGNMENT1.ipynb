{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'\\\\ad.susx.ac.uk\\ITS\\TeachingResources\\Departments\\Informatics\\LanguageEngineering\\resources')\n",
    "from IPython.display import display\n",
    "import random\n",
    "from random import seed\n",
    "from sussex_nltk.corpus_readers import AmazonReviewCorpusReader\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from collections import defaultdict,Counter\n",
    "from itertools import zip_longest\n",
    "from IPython.display import display\n",
    "get_ipython().magic('matplotlib inline')\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pylab as pylab\n",
    "%matplotlib inline\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'large',\n",
    "         'axes.titlesize':'large',\n",
    "         'xtick.labelsize':'large',\n",
    "         'ytick.labelsize':'large'}\n",
    "pylab.rcParams.update(params)\n",
    "from pylab import rcParams\n",
    "from operator import itemgetter, attrgetter, methodcaller\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from classification_utils import * \n",
    "\n",
    "dvd_reviews = [review for review in AmazonReviewCorpusReader().category(\"dvd\").raw()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Document Level Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Naive-Bayes Classifier has better performance than a wordlist classifier for determining the sentiment of a document.\n",
    "\n",
    "Both classifiers use labelled data to learn but the difference is in the way they perform the analysis of an unseen document.\n",
    "\n",
    "The wordlist classifier makes a basic count of whether each word is in a positive or negative list of words. Based on whether the count is positive or negative, it determines the sentiment. Accuracy is not always right because it chooses randomly if count is zero. Moreover, sometimes a sentence might have more words from the negative list, but the overall opinion might still be positive.\n",
    "\n",
    "The Naive-Bayes Classifier has a different approach. It uses probability to classify a document. It uses Bayes' rule, class priors and conditional probabilities to make the necessary calculations. It also uses a known vocabulary which can be always expanded. It performs better because there is more sophisticated maths in the way it's built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Word List</td>\n",
       "      <td>0.631333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NB</td>\n",
       "      <td>0.789000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1\n",
       "0  Word List  0.631333\n",
       "1         NB  0.789000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dvd_reader = AmazonReviewCorpusReader().category(\"dvd\")\n",
    "word_list_size = 100\n",
    "repetitions = 5 # accuracy figures are averaged over this many repetitions\n",
    "WL_accuracy_tot = 0\n",
    "NB_accuracy_tot = 0\n",
    "for i in range(repetitions): # for each sample_size we will find average accuracy over several repetitions\n",
    "    pos_train,neg_train,pos_test,neg_test = get_train_test_data(dvd_reader)\n",
    "    WL_accuracy_tot += run_WL(pos_train,neg_train,pos_test,neg_test,word_list_size)\n",
    "    NB_accuracy_tot += run_NB(pos_train,neg_train,pos_test,neg_test)\n",
    "WL_accuracy = WL_accuracy_tot/repetitions\n",
    "NB_accuracy = NB_accuracy_tot/repetitions\n",
    "df = pd.DataFrame([(\"Word List\",WL_accuracy),(\"NB\",NB_accuracy)])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe below shows how the Naive-Bayes Classifier improves when we increase the amount of data. We can see that its performance becomes better overall, which is logical since increasing the data, we increase the training data, or the known vocabulary, so it has a better chance to make correct analysis. However, in some cases it stays the same or even becomes worse. There is a big difference in its performance on a sample of one document compared to a sample of 100, but not that much from 400 to 700 for instance. Following from there, I think that if we increase the data massively, its performance wouldn't go much beyond 0.85 - 0.90.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample size</th>\n",
       "      <th>WL accuracy</th>\n",
       "      <th>NB accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>400</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>600</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>700</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sample size  WL accuracy  NB accuracy\n",
       "0            1         0.53         0.54\n",
       "1           10         0.53         0.55\n",
       "2           50         0.62         0.68\n",
       "3          100         0.62         0.71\n",
       "4          200         0.66         0.77\n",
       "5          400         0.65         0.80\n",
       "6          600         0.69         0.81\n",
       "7          700         0.68         0.85"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reader = AmazonReviewCorpusReader().category(\"kitchen\")\n",
    "word_list_size = 100\n",
    "repetitions = 1 # accuracy figures are averaged over this many repetitions\n",
    "sample_sizes = [1,10,50,100,200,400,600,700] #sample_size = number of the positive reviews =  number of negative reviews\n",
    "WL_accuracies=[]\n",
    "NB_accuracies=[]\n",
    "for size in sample_sizes:\n",
    "    WL_accuracy_tot = 0\n",
    "    NB_accuracy_tot = 0\n",
    "    for i in range(repetitions): # for each sample_size we will find average accuracy over several repetitions\n",
    "        pos_train,neg_train,pos_test,neg_test = get_train_test_data(reader)\n",
    "        pos_train_sample = sample(pos_train, size) \n",
    "        neg_train_sample = sample(neg_train, size) \n",
    "        WL_accuracy_tot += run_WL(pos_train_sample,neg_train_sample,pos_test,neg_test,word_list_size)\n",
    "        NB_accuracy_tot += run_NB(pos_train_sample,neg_train_sample,pos_test,neg_test)\n",
    "    WL_accuracies.append(WL_accuracy_tot/repetitions)\n",
    "    NB_accuracies.append(NB_accuracy_tot/repetitions)\n",
    "\n",
    "pd.set_option('precision',2)\n",
    "df = pd.DataFrame(list(zip(sample_sizes, WL_accuracies, NB_accuracies)),\n",
    "                  columns=[\"Sample size\",\"WL accuracy\",\"NB accuracy\"])    \n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigation of the impact on classifier accuracy of training a classifier with data in one domain (the source domain), and testing the same classifier on data from a different domain (the target domain) shows that some domain pairs lead to better accuracy than others. This might be caused by the different levels of similarity that domains have. The book and DVD domains have more in common in terms of vocabulary than book and electronics for instance. The Naive-Bayes Classifier creates a known vocabulary out of training data and then taking words from the testing data, it ignores these that are not present in its vocabulary. This means that it would ignore more words if the testing domain is a lot different and this will lead to worse accuracy overall. If the domains are similar, chances are they have more words in common, which means the classifier will make more use of its training. Of course, its accuracy will always be the best if using the same domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source Category</th>\n",
       "      <th>Target Category</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>book</td>\n",
       "      <td>book</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>book</td>\n",
       "      <td>dvd</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>book</td>\n",
       "      <td>electronics</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dvd</td>\n",
       "      <td>dvd</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dvd</td>\n",
       "      <td>book</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dvd</td>\n",
       "      <td>electronics</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dvd</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>electronics</td>\n",
       "      <td>electronics</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>electronics</td>\n",
       "      <td>book</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>electronics</td>\n",
       "      <td>dvd</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>electronics</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>kitchen</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>kitchen</td>\n",
       "      <td>book</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>kitchen</td>\n",
       "      <td>dvd</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>kitchen</td>\n",
       "      <td>electronics</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Source Category Target Category  Accuracy\n",
       "0             book            book      0.92\n",
       "1             book             dvd      0.73\n",
       "2             book     electronics      0.64\n",
       "3             book         kitchen      0.65\n",
       "4              dvd             dvd      0.92\n",
       "5              dvd            book      0.74\n",
       "6              dvd     electronics      0.64\n",
       "7              dvd         kitchen      0.67\n",
       "8      electronics     electronics      0.92\n",
       "9      electronics            book      0.69\n",
       "10     electronics             dvd      0.69\n",
       "11     electronics         kitchen      0.76\n",
       "12         kitchen         kitchen      0.93\n",
       "13         kitchen            book      0.67\n",
       "14         kitchen             dvd      0.69\n",
       "15         kitchen     electronics      0.77"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "categories_pairs = [(\"book\", \"book\"), (\"book\", \"dvd\"), (\"book\", \"electronics\"), (\"book\",  \"kitchen\"),\n",
    "                    (\"dvd\", \"dvd\"), (\"dvd\", \"book\"), (\"dvd\", \"electronics\"), (\"dvd\", \"kitchen\"),\n",
    "                    (\"electronics\", \"electronics\"), (\"electronics\", \"book\"), (\"electronics\", \"dvd\"), (\"electronics\", \"kitchen\"),\n",
    "                    (\"kitchen\", \"kitchen\"), (\"kitchen\", \"book\"), (\"kitchen\", \"dvd\"), (\"kitchen\", \"electronics\")]\n",
    "pair_accuracies = []\n",
    "for cat in categories_pairs:\n",
    "    repetitions = 3\n",
    "    pair_accuracy_tot = 0\n",
    "    for i in range(repetitions):\n",
    "        source_test, source_train = get_formatted_train_test_data(cat[0])\n",
    "        target_test, target_train = get_formatted_train_test_data(cat[1])\n",
    "        pair_accuracy_tot += run_NB_preformatted(source_train, target_test)\n",
    "    pair_accuracies.append(pair_accuracy_tot/repetitions)\n",
    "df = pd.DataFrame(list(zip([cat[0] for cat in categories_pairs], [cat[1] for cat in categories_pairs], pair_accuracies)),\n",
    "                 columns = [\"Source Category\", \"Target Category\", \"Accuracy\"])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "By using different feature extraction methods we can see how the accuracy of the classifier changes. It's highest when we use all of the tokens in book and kitchen domain. In dvd the classifier is performing the best when we've filtered out non-alphabetic words and stopwords. In electronics its accuracy is highest when we've converted all tokens to lowercase. It's quite surprising that filtering numbers and stopwords does not lead to highest accuracy in all of the domains. It's even lowest in book and electronics. Converting tokens to NUM does not make much difference in most of the domains except in electronics. This might follow from the fact that in these revies there are a lot of numbers present (defining models and characteristics of different products for example) that make analysis more difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>all</th>\n",
       "      <th>lower</th>\n",
       "      <th>NUM</th>\n",
       "      <th>puncstop</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>book</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dvd</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kitchen</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>electronics</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           cat   all  lower   NUM  puncstop  stem\n",
       "0         book  0.76   0.77  0.77      0.78  0.79\n",
       "1          dvd  0.79   0.81  0.79      0.81  0.78\n",
       "2      kitchen  0.84   0.81  0.82      0.81  0.82\n",
       "3  electronics  0.80   0.80  0.80      0.80  0.80"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x233f914a2e8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAGMCAYAAADz6So5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDEbhLSIiYjAKbxEREYNReIuIiBjM/wOgET1coGTEQgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x233f9565780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def FE_all(review):\n",
    "    return review.words()\n",
    "\n",
    "def FE_lower(review):\n",
    "    return [word.lower() for word in review.words()]\n",
    "\n",
    "def FE_NUM(review):\n",
    "    return [\"NUM\" if word.isdigit() else word for word in review.words()]\n",
    "\n",
    "def FE_puncstop(review):\n",
    "    return [word for word in review.words() if word.isalpha() and word not in stopwords]\n",
    "\n",
    "def FE_stem(review):\n",
    "    return [stemmer.stem(word) for word in review.words()]\n",
    "\n",
    "def get_results(feature_extractor):\n",
    "    results = {}\n",
    "    for prod_cat in prod_cats:\n",
    "        repetitions = 3 \n",
    "        NB_accuracy_tot = 0\n",
    "        for i in range(repetitions): \n",
    "            test, train   = get_formatted_train_test_data(prod_cat,feature_extractor)\n",
    "            NB_accuracy_tot += run_NB_preformatted(train,test)\n",
    "        results[prod_cat] = NB_accuracy_tot/repetitions\n",
    "    return results\n",
    "\n",
    "prod_cats = [\"book\",\"dvd\",\"kitchen\",\"electronics\"]\n",
    "FE_all_results = get_results(FE_all)\n",
    "FE_lower_results = get_results(FE_lower)\n",
    "FE_NUM_results = get_results(FE_NUM)\n",
    "FE_puncstop_results = get_results(FE_puncstop)\n",
    "FE_stem_results = get_results(FE_stem)\n",
    "\n",
    "headers = [\"cat\",\"all\",\"lower\", \"NUM\", \"puncstop\", \"stem\"]\n",
    "\n",
    "pd.set_option('precision',2)\n",
    "\n",
    "df = pd.DataFrame(list(zip(prod_cats,\n",
    "                           [FE_all_results[prod_cat] for prod_cat in prod_cats],\n",
    "                           [FE_lower_results[prod_cat] for prod_cat in prod_cats],\n",
    "                           [FE_NUM_results[prod_cat] for prod_cat in prod_cats],\n",
    "                           [FE_puncstop_results[prod_cat] for prod_cat in prod_cats],\n",
    "                           [FE_stem_results[prod_cat] for prod_cat in prod_cats])),\n",
    "                  columns=headers)\n",
    "display(df)\n",
    "ax = df.plot.bar(x=0,title=\"Experimental Results\")\n",
    "ax.set_ylabel(\"Classifier Accuracy\")\n",
    "ax.set_xlabel(\"Product Category\")\n",
    "ax.set_ylim(0.5,1.0)\n",
    "ax.legend(bbox_to_anchor=(0.95, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Opinion Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In my opinion_extractor I have a main method that deals with all extensions but conjunction which is separated as a separate 'conj_method'.\n",
    "\n",
    "The global variables created in the two methods are counters for the number of sentences where each extension applied. They are for reporting later in Section 3.2 but I found it convenient to put them in the methods here.\n",
    "\n",
    "'opinion_extractor' takes two arguments: aspect_token and parsed_sentence. It creates an empty list 'opinions' that keeps track of our opinion bearing words. A boolean 'neg' will mark the sentence as negative or not.\n",
    "\n",
    "The method consists of two big for-loops that deal with two cases: we either have an aspect word with adjectives to it, or we have a form of 'to be' to which an adjective describing the aspect word is linked.\n",
    "\n",
    "The first for-loop deals with the first case. It searches through all the children of the aspect word. It creates a new list 'phrases' to keep track of possible adverb+adjective phrases and negations.\n",
    "\n",
    "If child is negative, it marks the sentence as negative by making the 'neg' boolean true.\n",
    "\n",
    "If child has a dependency 'amod' and is an adjective, then it's the opinion bearing word we need. We add it to 'phrases'. Then we search through its children to find conjunctions or adverbs.\n",
    "\n",
    "We use the 'conj_method' to search for conjunctions and then add them to our 'opinions' list. An explanation of it will follow below.\n",
    "\n",
    "If we find a token with the dependency 'advmod', we've found our adverb. We make sure that if there is another adverb in our 'phrases', we put the new one in-between. I will take as an example the sentence \"The plot for the movie is actually surprisingly straightforward once you cut away all the filler, but that's the biggest problem.\" If we don't have the if-statement explained above, the output will be 'surprisingly-actually-straightforward' which doesn't sound right. Normally the first adverb we find during the iteration is explanatory for the second one, so it needs to be at the front.\n",
    "\n",
    "If we've marked our sentence as negative, we add 'not' to each adjective in 'phrases' and we append it to 'opinions'. If it's positive, we have two possibilities. First case, 'phrases' has more than one element which means it has an adverb and an adjective: we join the words with hyphens and then add them to 'opinions'. Second case, 'phrases' has only one item in it - an adjective: we simply add it to our 'opinions' list. The if-statement before appending makes sure the word added is an adjective and not a determinant (it's not an opinion bearing word, is it's a 'the').\n",
    "\n",
    "The second for-loop deals with the case where we have a form of 'to be'. We search through all of the children of the verb. We create a new 'phrases' list to keep track of words we add. If we find a word with the dependency 'acomp', we've found an adjective or our opinion-bearing word. We add it to 'phrases'. Then we go into another for-loop to search through that token's children to cover conjunctions, adverbs and negations. We deal with conjunctions in the separate method. If we find an adverb, we check for the length of 'phrases': if it has only one item in it, we add it at the front of the list. If it has two, it means we have a 'not', so we make sure we add the adverb in-between. If we find a negation, we add 'not' at the front of the list. At the end we check for length of 'phrases' again to make sure we add hyphens if we have more than one word. Otherwise, we simply add the adjective to 'opinions'.\n",
    "\n",
    "The 'conj_method' mirrors the 'opinion_extractor' method. It takes one argument: the token we need to check. It has two lists to keep track of everything we add and a boolean for negation. If the token has a dependency 'conj', we add it to 'conj' list. Then we search through its children. If we find another 'conj' dependency, we add it to the list. If we find an adverb, we make sure we add it at the front. If we find a 'neg' dependency, we mark the sentence as negative. If we have a non-empty list and 'conj_neg' is true, we add a 'not' in the list, concatenate everything with hyphens and add it to the main list. If 'conj_neg' is false, we simply add 'conj' to the main list. At the end we return the main list that's to be later appended to the 'opinions' list in 'opinion_extractor'.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#method for conjunction\n",
    "def conj_method(child_of_adjective):\n",
    "    global conj_ext_num #counter for use of extension E\n",
    "    ops = []\n",
    "    conj = []\n",
    "    conj_neg = False\n",
    "    if child_of_adjective.dep_ == 'conj':\n",
    "        conj_ext_num += 1\n",
    "        conj.append(child_of_adjective.orth_)\n",
    "        for child_of_conj in child_of_adjective.children:\n",
    "            if child_of_conj.dep_ == 'conj': #we search for more conjunctions\n",
    "                conj.append(child_of_conj.orth_)\n",
    "            if child_of_conj.dep_ == 'advmod':\n",
    "                conj.insert(0, child_of_conj.orth_) #we make sure we insert the adverb in front of the adjective\n",
    "            if child_of_conj.dep_ == 'neg':\n",
    "                conj_neg = True #mark conjunction as negative\n",
    "        if conj and conj_neg: #if we have negative conjunctions, make a joined negative string\n",
    "            conj.insert(0, \"not\")\n",
    "            joined_string = \"-\".join(conj)\n",
    "            ops.append(joined_string)\n",
    "        elif conj:\n",
    "            ops.extend(conj)\n",
    "    return ops\n",
    "\n",
    "def opinion_extractor(aspect_token,parsed_sentence):\n",
    "    global amod_ext_num #counter for use of extension A\n",
    "    global acomp_ext_num #counter for use of extension B\n",
    "    global advmod_ext_num #counter for use of extension C\n",
    "    global neg_ext_num #counter for use of extension D\n",
    "    opinions = []\n",
    "    neg = False\n",
    "    for token in parsed_sentence:\n",
    "        if token.pos_ == 'NOUN' and token.orth_ == aspect_token:\n",
    "            for child in token.children: #case 1: search for adjectives for aspect_token\n",
    "                phrases = []\n",
    "                if child.dep_ == 'neg':\n",
    "                    neg = True\n",
    "                if child.dep_ == 'amod' and child.pos_ == 'ADJ': \n",
    "                    amod_ext_num += 1\n",
    "                    phrases.append(child.orth_)\n",
    "                    for child_of_adjective in child.children:\n",
    "                        opinions.extend(conj_method(child_of_adjective)) #check for conjunctions\n",
    "                        if child_of_adjective.dep_ == 'advmod':\n",
    "                            advmod_ext_num += 1\n",
    "                            #check for length so as to check if there is more than one adverb in phrases: if yes, \n",
    "                            #need to insert the adverb between the two words.\n",
    "                            if len(phrases) == 1:\n",
    "                                phrases.insert(0, child_of_adjective.orth_)\n",
    "                            else:\n",
    "                                phrases.insert(1, child_of_adjective.orth_)   \n",
    "                if neg:\n",
    "                    neg_ext_num += 1\n",
    "                    #if we have a negation, add 'not' for each adjective in phrases and add in opinions\n",
    "                    for i in range(0, len(phrases)):\n",
    "                        opinions.append(\"-\".join([\"not\", phrases[i]]))\n",
    "                else:\n",
    "                    #if we have more than one word in phrases, this means we have an adverb, so add '-' between words\n",
    "                    #before adding the phrase to opinions\n",
    "                    if len(phrases) > 1:\n",
    "                        joined_string = \"-\".join(phrases)\n",
    "                        opinions.append(joined_string)\n",
    "                    #else, just add the adjective in opinions\n",
    "                    else:\n",
    "                        if child.pos_ == 'ADJ': #we make sure we don't add determinants\n",
    "                            opinions.append(child.orth_)   \n",
    "            for child_of_head in token.head.children: #case 2: search through children of the head(form of to be)\n",
    "                phrases = []\n",
    "                adj_neg = False\n",
    "                if child_of_head.dep_ == 'acomp':\n",
    "                    acomp_ext_num += 1\n",
    "                    phrases.append(child_of_head.orth_)                    \n",
    "                    for child_of_adjective in child_of_head.children:\n",
    "                        opinions.extend(conj_method(child_of_adjective))\n",
    "                        if child_of_adjective.dep_ == 'advmod':\n",
    "                            advmod_ext_num += 1\n",
    "                            if len(phrases) == 1:\n",
    "                                phrases.insert(0, child_of_adjective.orth_)\n",
    "                            else:\n",
    "                                phrases.insert(1, child_of_adjective.orth_)\n",
    "                        if child_of_adjective.dep_ == 'neg':\n",
    "                            neg_ext_num += 1\n",
    "                            phrases.insert(0, \"not\")\n",
    "                    if len(phrases) > 1:    \n",
    "                        joined_string = \"-\".join(phrases)\n",
    "                        opinions.insert(0, joined_string)\n",
    "                    else:\n",
    "                        opinions.insert(0, child_of_head.orth_)\n",
    "    return opinions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for aspect word 'plot'\n",
      "\n",
      "Sentence:\n",
      "\tIt has an exciting fresh plot.\n",
      "The opinion should be:\n",
      "\t ['exciting', 'fresh']\n",
      "Opinion of 'plot':\n",
      "\t 'exciting', 'fresh'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tThe plot was dull.\n",
      "The opinion should be:\n",
      "\t dull\n",
      "Opinion of 'plot':\n",
      "\t 'dull'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tIt has an excessively dull plot.\n",
      "The opinion should be:\n",
      "\t excessively-dull\n",
      "Opinion of 'plot':\n",
      "\t 'excessively-dull'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tThe plot was excessively dull.\n",
      "The opinion should be:\n",
      "\t excessively-dull\n",
      "Opinion of 'plot':\n",
      "\t 'excessively-dull'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tThe plot wasn't dull.\n",
      "The opinion should be:\n",
      "\t not-dull\n",
      "Opinion of 'plot':\n",
      "\t 'not-dull'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tIt wasn't an exciting fresh plot.\n",
      "The opinion should be:\n",
      "\t ['not-exciting', 'not-fresh']\n",
      "Opinion of 'plot':\n",
      "\t 'not-exciting', 'not-fresh'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tThe plot wasn't excessively dull.\n",
      "The opinion should be:\n",
      "\t not-excessively-dull\n",
      "Opinion of 'plot':\n",
      "\t 'not-excessively-dull'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tThe plot was cheesy, but fun and inspiring.\n",
      "The opinion should be:\n",
      "\t ['cheesy', 'fun', 'inspiring']\n",
      "Opinion of 'plot':\n",
      "\t 'cheesy', 'fun', 'inspiring'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tThe plot was really cheesy and not particularly special.\n",
      "The opinion should be:\n",
      "\t ['really-cheesy', 'not-particularly-special']\n",
      "Opinion of 'plot':\n",
      "\t 'really-cheesy', 'not-particularly-special'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ext_num = 0\n",
    "amod_ext_num = 0\n",
    "acomp_ext_num = 0\n",
    "advmod_ext_num = 0\n",
    "neg_ext_num = 0\n",
    "conj_ext_num = 0\n",
    "\n",
    "core_sentences = [(\"It has an exciting fresh plot.\", [\"exciting\", \"fresh\"]), \n",
    "        (\"The plot was dull.\", \"dull\"),\n",
    "        (\"It has an excessively dull plot.\", \"excessively-dull\"),\n",
    "        (\"The plot was excessively dull.\", \"excessively-dull\"),\n",
    "        (\"The plot wasn't dull.\", \"not-dull\"),\n",
    "        (\"It wasn't an exciting fresh plot.\", [\"not-exciting\", \"not-fresh\"]),\n",
    "        (\"The plot wasn't excessively dull.\", \"not-excessively-dull\"),\n",
    "        (\"The plot was cheesy, but fun and inspiring.\", [\"cheesy\", \"fun\", \"inspiring\"]),\n",
    "        (\"The plot was really cheesy and not particularly special.\", [\"really-cheesy\", \"not-particularly-special\"])]\n",
    "\n",
    "def show_results(results,aspect_word):\n",
    "    print(\"Results for aspect word '{}'\\n\".format(aspect_word))\n",
    "    for word,sent,opinions,answer in results:\n",
    "        if word == aspect_word:\n",
    "            print(\"Sentence:\\n\\t{}\".format(sent))\n",
    "            print(\"The opinion should be:\\n\\t {0}\".format(answer))\n",
    "            print(\"Opinion of '{0}':\\n\\t '{1}'\".format(aspect_word,\"', '\".join(opinions)))\n",
    "            print(\"\\n\")\n",
    "                \n",
    "aspect_words = [\"plot\",\"characters\",\"cinematography\",\"dialogue\"]\n",
    "\n",
    "results = [] \n",
    "for review, answer in core_sentences:\n",
    "    parsed_review = nlp(review)\n",
    "    for sentence in parsed_review.sents:\n",
    "        for aspect_token in aspect_words:\n",
    "            opinions = opinion_extractor(aspect_token,sentence)\n",
    "            if opinions:\n",
    "                results.append((aspect_token,sentence.orth_,opinions, answer))\n",
    "\n",
    "show_results(results,\"plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little side note: I have over-complicated my method by making it return opinion words in the same order in the core set. That's why here I haven't created a set for them, but just a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Assessment of Opinion Extractor Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed(167184)\n",
    "\n",
    "def target_sentence(sentence,target_tokens):\n",
    "    for token in sentence:\n",
    "        if token.orth_ in target_tokens:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "target_tokens = {\"plot\",\"characters\",\"cinematography\",\"dialogue\"}\n",
    "sample_size = 100\n",
    "my_sample = []\n",
    "num_found = 0\n",
    "while num_found < sample_size:\n",
    "    review = random.choice(dvd_reviews)\n",
    "    parsed_review = nlp(review)\n",
    "    sentence = random.choice(list(parsed_review.sents))\n",
    "    if target_sentence(sentence,target_tokens):\n",
    "        my_sample.append(sentence)\n",
    "        num_found += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences with 'plot': 37\n",
      "Number of sentences with 'characters': 48\n",
      "Number of sentences with 'cinematography': 1\n",
      "Number of sentences with 'dialogue': 18\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def number_of_aspect_words(sample):\n",
    "    num_plot = 0\n",
    "    num_characters = 0\n",
    "    num_cinematography = 0\n",
    "    num_dialogue = 0\n",
    "    for sentence in sample:\n",
    "        for token in sentence:\n",
    "            if token.orth_ == \"plot\":\n",
    "                num_plot +=1\n",
    "            if token.orth_ == \"characters\":\n",
    "                num_characters += 1\n",
    "            if token.orth_ == \"cinematography\":\n",
    "                num_cinematography += 1\n",
    "            if token.orth_ == \"dialogue\":\n",
    "                num_dialogue += 1  \n",
    "    nums = [num_plot, num_characters, num_cinematography, num_dialogue]\n",
    "    for aspect_word, number in zip(aspect_words, nums):\n",
    "        print(\"Number of sentences with '{0}': {1}\".format(aspect_word, number))\n",
    "    \n",
    "    \n",
    "number_of_aspect_words(my_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you will see after executing the cell above, sentences with 'plot' are 37, with 'characters' - 48, with 'cinematography' - 1, and with 'dialogue' - 18. The sum of all of them is 104 and not 100, because two of the sentences include both 'plot' and 'characters' which means they are each counted twice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for aspect word 'plot'\n",
      "\n",
      "Sentence:\n",
      "\tThe similarities of these 2 movies are so prominent that it would actually be a waste of time and money to buy both - same characters, same plot only in a different setting.\n",
      "Opinion of 'plot':\n",
      "\t 'same'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tAs far as the movie is concerned, the plot is unoriginal and not particularly engaging.\n",
      "Opinion of 'plot':\n",
      "\t 'unoriginal', 'not-particularly-engaging'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tit was an ight movie..had lots of action, pretty gory compared to other Jet li movies.. the movie has a cheesy plot..but the action covers it all.. the fighting in the movie cant be compared to that of Fist of legend (wich in my oponion is Jet Li's best work) but its still adds up to great action..one things for shure, this aint really a hard core martial arts movie..theres is alot of shootings..stabbings, killing..gory scenes and sum distrubing scenes..pretty dark for a jet li movie..thats why i kina understand the \"R\" rating.. the swearing is really fun too..because the movie is dubbed horribly..but its still ok..like when they say muther effer..in one scene a crime boss says \" we better get that muther effer\" lol i laughed,,there is really corny lines through out the movie..i guess it gives it comic relief..as the movie is a little too serious...the hip hop in the movie also pissed me off, they actually thought it was kool to slip sum rap tracks in the background..well it made it annoying... some simple music woulda been good., overall the movie is good, hell even ma dad liked it..dont think..Jet li has made better crap then this..if u wana waste time n watch a movie..watch this....ight im out..peac\n",
      "Opinion of 'plot':\n",
      "\t 'cheesy'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tFor a more explosive ride, with a more entertaining character, and a much simpler drug-induced and graphic plot, see the recently released Crank - excellent action/adventure from the Transporter man.\n",
      "Opinion of 'plot':\n",
      "\t 'much-simpler', 'graphic'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tGood plot.\n",
      "Opinion of 'plot':\n",
      "\t 'Good'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tThe plot is stupid, the characters are dull and insipid, and the whole movie leaves you asking, as the credits run, \"So What?\" Don't waste your money!\n",
      "Opinion of 'plot':\n",
      "\t 'stupid'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tI enjoyed this movie because the plot was very different and not run of the mill like most horror movies.\n",
      "Opinion of 'plot':\n",
      "\t 'very-different'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tit was an ight movie..had lots of action, pretty gory compared to other Jet li movies.. the movie has a cheesy plot..but the action covers it all.. the fighting in the movie cant be compared to that of Fist of legend (wich in my oponion is Jet Li's best work) but its still adds up to great action..one things for shure, this aint really a hard core martial arts movie..theres is alot of shootings..stabbings, killing..gory scenes and sum distrubing scenes..pretty dark for a jet li movie..thats why i kina understand the \"R\" rating.. the swearing is really fun too..because the movie is dubbed horribly..but its still ok..like when they say muther effer..in one scene a crime boss says \" we better get that muther effer\" lol i laughed,,there is really corny lines through out the movie..i guess it gives it comic relief..as the movie is a little too serious...the hip hop in the movie also pissed me off, they actually thought it was kool to slip sum rap tracks in the background..well it made it annoying... some simple music woulda been good., overall the movie is good, hell even ma dad liked it..dont think..Jet li has made better crap then this..if u wana waste time n watch a movie..watch this....ight im out..peac\n",
      "Opinion of 'plot':\n",
      "\t 'cheesy'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tVery good plot and detail\n",
      "Opinion of 'plot':\n",
      "\t 'good'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tIt was not the actual story/plot, or it's accuracy that was important to me.\n",
      "Opinion of 'plot':\n",
      "\t 'actual'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tI think the director needs prozac to direct this movie, the whole plot is so mess up, junior high student can write better story than this..what was he thinking. don't waste your money buying this movi\n",
      "Opinion of 'plot':\n",
      "\t 'whole'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tThe plot is too implausible and riddled with inconsistencies to be taken literally.\n",
      "Opinion of 'plot':\n",
      "\t 'too-implausible', 'riddled'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tIt combines a bunch of the best French actrices, and an enjoyable plot.\n",
      "Opinion of 'plot':\n",
      "\t 'enjoyable'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tMore blood, nudity, violence, swearing, and sex than the first movie, less plot and characterization.\n",
      "Opinion of 'plot':\n",
      "\t 'less'\n",
      "\n",
      "\n",
      "Results for aspect word 'characters'\n",
      "\n",
      "Sentence:\n",
      "\tThe similarities of these 2 movies are so prominent that it would actually be a waste of time and money to buy both - same characters, same plot only in a different setting.\n",
      "Opinion of 'characters':\n",
      "\t 'both-same'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tIt's for anyone who has a longing to \"escape\" the everyday just for awhile...just a fun story, charming characters, and a wonderful love story that makes you want to fly off to Rome\n",
      "Opinion of 'characters':\n",
      "\t 'charming'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tThis one has it all- a good storyline, good acting, memorable characters, and introduces our favorite killer- Jason\n",
      "Opinion of 'characters':\n",
      "\t 'good', 'memorable'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tThe plot is stupid, the characters are dull and insipid, and the whole movie leaves you asking, as the credits run, \"So What?\" Don't waste your money!\n",
      "Opinion of 'characters':\n",
      "\t 'dull', 'insipid'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tIf you want to watch an amazing movie get this movie and watch it because you will never see anything like this ever again russel's performance is incredible and denzel performence is also incredble over all this movie is one helleva movie because they both play very complex characters and they do very good jobs you won't see the sadistic and actual enjoyment out of killing people that you see here anywhere else get this movie it is a keeper I personally like the sadistic demeanor and uncontrollable carnage in the movie have fun watching it\n",
      "Opinion of 'characters':\n",
      "\t 'very-complex'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tIt's characters are re-created after Fellini's cartoons, or stereotypes (some based on real characters in Rimini), some comlpetely imaginary, all most funny and entertaining.\n",
      "Opinion of 'characters':\n",
      "\t 'real'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tCaught between these two dynamic characters are the soldiers who bleed and die during these pair's struggles against each other.\n",
      "Opinion of 'characters':\n",
      "\t 'dynamic'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tThe other characters (teenagers) are a \"fag hag\", a cross dresser, a chubby girl who has her mouth wired shut, a bleached blonde who thinks too highly of herself and her meek sidekick and a rare straight boy at the camp who lacks self-esteem.\n",
      "Opinion of 'characters':\n",
      "\t 'other'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tThis one has it all- a good storyline, good acting, memorable characters, and introduces our favorite killer- Jason\n",
      "Opinion of 'characters':\n",
      "\t 'good', 'memorable'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tThere is no humor, no likable characters, not even a decent quote or one liner.   by the time it ambles toward its pathetic excuse for a conclusion\n",
      "Opinion of 'characters':\n",
      "\t 'likable'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tThere are many good supporting characters such as: Bridget Fonda, Woody Harrelson, Barnard Hughes and Julie Warner\n",
      "Opinion of 'characters':\n",
      "\t 'many', 'good'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tThe characters are very well developed and outlandish from the eccentricly outgoing berry(Jack Black) to the quiet weird geek type Dick.\n",
      "Opinion of 'characters':\n",
      "\t 'well-developed', 'outlandish'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tThe props and things were very nice, the actors didn't gel well with each other or connect with their characters and the singing was mediocre ( definitely no Tyler Perry quality on that note) but the messages were good just seemed a little unrealistic and too drawn out.\n",
      "Opinion of 'characters':\n",
      "\t 'their'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tShe is a great alternative to Barney and all of the other commercial characters, who I don't care much for.\n",
      "Opinion of 'characters':\n",
      "\t 'other', 'commercial'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tThe big problem with RH is that neither Ford or Benning are sympathetic characters.\n",
      "Opinion of 'characters':\n",
      "\t 'sympathetic'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tThe props and things were very nice, the actors didn't gel well with each other or connect with their characters and the singing was mediocre ( definitely no Tyler Perry quality on that note) but the messages were good just seemed a little unrealistic and too drawn out.\n",
      "Opinion of 'characters':\n",
      "\t 'their'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tAlso, all the characters knew of the dangerous gingerbread man in the bakery but they never left to get help, it was as easy as opening the door and leaving, but no...can't do that.\n",
      "Opinion of 'characters':\n",
      "\t 'all'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tIt's a shame they don't show this on television anymore, so few shows today exude a sense of genuine concern for its characters as Degrassi did\n",
      "Opinion of 'characters':\n",
      "\t 'its'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\t**1/2\n",
      "\n",
      "If \"North Country\" didn't say that it was set in 1989, you'd never believe it based on the attitudes and behavior of its male characters, which can only be classified as \"Neanderthal\" (though that might be an insult to Neanderthals).\n",
      "Opinion of 'characters':\n",
      "\t 'its', 'male'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tThe singing is not bad (this cast has better technical control of their voices compared to the cast in Les Miz) but the choreography is trite and unimaginative, and the acting so juvenille that you'd think the characters are retarded.\n",
      "Opinion of 'characters':\n",
      "\t 'retarded'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tDepth in its story, depth in its characters, and this is why people of all age groups (I myself am 22+) will love this show.\n",
      "Opinion of 'characters':\n",
      "\t 'its'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tThis is a film that deserves wide audience exposure, and especially for those young people who are struggling with their sexuality in the ugly isolation surrounding the lives of the main characters of this excellent film.\n",
      "Opinion of 'characters':\n",
      "\t 'main'\n",
      "\n",
      "\n",
      "Results for aspect word 'cinematography'\n",
      "\n",
      "Sentence:\n",
      "\tThis film has it all, great performances, great direction, and great cinematography.\n",
      "Opinion of 'cinematography':\n",
      "\t 'great'\n",
      "\n",
      "\n",
      "Results for aspect word 'dialogue'\n",
      "\n",
      "Sentence:\n",
      "\tso I'm supposed to believe that the horrible dialogue, bad acting, and ridiculous plotting is on purpose just because Mamet is a respected guy? how about we call a dog a dog and just move on\n",
      "Opinion of 'dialogue':\n",
      "\t 'horrible'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tThe dialogue is quick-witted, some the scenes have become classics (Allen sneezing on the cocaine).\n",
      "Opinion of 'dialogue':\n",
      "\t 'witted'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tThe dialogue is completely unbelievable considering the age of these kids.\n",
      "Opinion of 'dialogue':\n",
      "\t 'completely-unbelievable'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tThe movie is action packed with excellent dialogue and the action never stops until the very appropriate end.\n",
      "Opinion of 'dialogue':\n",
      "\t 'excellent'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tMy wife says all of his movies are the same, and it breaks my heart everytime she says it...because in that blanket analysis, the analyst misses the incredible stories and crafty dialogue that takes place in the many Woody Allen movies that pretty much take place within the same template.\n",
      "Opinion of 'dialogue':\n",
      "\t 'crafty'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tIt seems rare today to find such rich dialogue.\n",
      "Opinion of 'dialogue':\n",
      "\t 'such', 'rich'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tThis is a must-have if you like a good story line, clever dialogue and honest laughs.\n",
      "Opinion of 'dialogue':\n",
      "\t 'clever'\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\tThe story is filled with big laughs--from crude humor to some of the most intelligent dialogue I've probably ever heard.\n",
      "Opinion of 'dialogue':\n",
      "\t 'most-intelligent'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ext_num = 0\n",
    "amod_ext_num = 0\n",
    "acomp_ext_num = 0\n",
    "advmod_ext_num = 0\n",
    "neg_ext_num = 0\n",
    "conj_ext_num = 0\n",
    "\n",
    "results = [] \n",
    "for sentence in my_sample:\n",
    "    for aspect_token in aspect_words:\n",
    "        opinions = opinion_extractor(aspect_token,sentence)\n",
    "        if opinions:\n",
    "            ext_num += 1\n",
    "            results.append((aspect_token,sentence.orth_,opinions))\n",
    "\n",
    "show_results(results,\"plot\")\n",
    "show_results(results,\"characters\")\n",
    "show_results(results,\"cinematography\")\n",
    "show_results(results,\"dialogue\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences where opinion extractor worked: 45 \n",
      "\n",
      "Number of sentences where 'Extension A: Adjectival modification' was applied: 36\n",
      "\n",
      "Number of sentences where 'Extension B: Adjectives linked by copulae' was applied: 9\n",
      "\n",
      "Number of sentences where 'Extension C: Adverbial modifiers' was applied: 8\n",
      "\n",
      "Number of sentences where 'Extension D: Negation' was applied: 0\n",
      "\n",
      "Number of sentences where 'Extension E: Conjunction' was applied: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extensions = [(\"Extension A: Adjectival modification\"), (\"Extension B: Adjectives linked by copulae\"), \n",
    "             (\"Extension C: Adverbial modifiers\"), (\"Extension D: Negation\"), (\"Extension E: Conjunction\")]\n",
    "ext_nums = [amod_ext_num, acomp_ext_num, advmod_ext_num, neg_ext_num, conj_ext_num]\n",
    "print(\"Number of sentences where opinion extractor worked: {0} \\n\".format(ext_num))\n",
    "for ext, num in zip(extensions, ext_nums):\n",
    "    print(\"Number of sentences where '{0}' was applied: {1}\\n\".format(ext, num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Running the cell above will give indication of how many times each extension was applied.\n",
    "\n",
    "- Examples that include extension A are:\n",
    "\n",
    "\"This film has it all, great performances, great direction, and great cinematography.\"\n",
    "\n",
    "    Opinion of 'cinematography': 'great'\n",
    "    \n",
    "`Good plot.`\n",
    "\n",
    "    Opinion of 'plot':'Good'\n",
    "\n",
    "- Examples that include extension B are:\n",
    "\n",
    "\"The singing is not bad (this cast has better technical control of their voices compared to the cast in Les Miz) but the choreography is trite and unimaginative, and the acting so juvenille that you'd think the characters are retarded.\"\n",
    "\n",
    "    Opinion of 'characters': 'retarded'\n",
    "    \n",
    "\n",
    "    \n",
    "- Examples that include extension C are:\n",
    "    \n",
    "\"I enjoyed this movie because the plot was very different and not run of the mill like most horror movies.\"\n",
    "\n",
    "    Opinion of 'plot': 'very-different'\n",
    "\n",
    "\"The dialogue is completely unbelievable considering the age of these kids.\"\n",
    "\n",
    "    Opinion of 'dialogue': 'completely-unbelievable'\n",
    "    \n",
    "- Examples that include extension D are:\n",
    "\n",
    "\"As far as the movie is concerned, the plot is unoriginal and not particularly engaging.\"\n",
    "\n",
    "    Opinion of 'plot': 'unoriginal', 'not-particularly-engaging'\n",
    "    \n",
    "- Examples that include extension E are:\n",
    "    \n",
    "\"The plot is stupid, the characters are dull and insipid, and the whole movie leaves you asking, as the credits run, \"So What?\" Don't waste your money!\"\n",
    "\n",
    "    Opinion of 'characters': 'dull', 'insipid'\n",
    "    \n",
    "\"The characters are very well developed and outlandish from the eccentricly outgoing berry(Jack Black) to the quiet weird geek type Dick.\"\n",
    "\n",
    "    Opinion of 'characters': 'well-developed', 'outlandish'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The cell above also gives an overall number of how many of the sentences the opinion extractor worked for in the sense that it did not return an empty 'opinions' list. \n",
    "It is 45 out of 100 in the sample.\n",
    "\n",
    "The number of sentences which the extractor analysed correctly is much smaller. After going through the results of each sentence, I went to the conclusion that the opinion extractor analysed correctly 42.22% of the sentences it worked for. After taking into consideration the sentences it did not produce any output for at all, the proportion becomes 19%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Some of the sentences that were perfectly analysed are:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"As far as the movie is concerned, the plot is unoriginal and not particularly engaging.\"\n",
    "\n",
    "    Opinion of 'plot': 'unoriginal', 'not-particularly-engaging'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The plot is stupid, the characters are dull and insipid, and the whole movie leaves you asking, as the credits run, \"So What?\" Don't waste your money!\"\n",
    "\n",
    "    Opinion of 'characters': 'dull', 'insipid'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"It's for anyone who has a longing to \"escape\" the everyday just for awhile...just a fun story, charming characters, and a wonderful love story that makes you want to fly off to Rome\"\n",
    "\n",
    "    Opinion of 'characters':'charming'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"This is a must-have if you like a good story line, clever dialogue and honest laughs.\"\n",
    "\n",
    "    Opinion of 'dialogue': 'clever'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"If you want to watch an amazing movie get this movie and watch it because you will never see anything like this ever again russel's performance is incredible and denzel performence is also incredble over all this movie is one helleva movie because they both play very complex characters and they do very good jobs you won't see the sadistic and actual enjoyment out of killing people that you see here anywhere else get this movie it is a keeper I personally like the sadistic demeanor and uncontrollable carnage in the movie have fun watching it\"\n",
    "\n",
    "    Opinion of 'characters': 'very-complex'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### In many cases the opinion extractor did not work properly. These include:\n",
    "\n",
    "\"For a more explosive ride, with a more entertaining character, and a much simpler drug-induced and graphic plot, see the recently released Crank - excellent action/adventure from the Transporter man.\"\n",
    "\n",
    "    Opinion of 'plot': 'much-simpler', 'graphic'\n",
    "\n",
    "- In this case 'induced' here has a dependency 'amod', but its tag is verb, and in my opinion-extractor I'm only searching for adjectives.\n",
    "\n",
    "\n",
    "\"It was not the actual story/plot, or it's accuracy that was important to me.\"\n",
    "\n",
    "    Opinion of 'plot': 'actual'\n",
    "\n",
    "- The first mistake here: actual is not an opinion bearing word. This has happened in a couple of other sentences as well like: \"The other characters (teenagers) are a \"fag hag\", a cross dresser, a chubby girl who has her mouth wired shut, a bleached blonde who thinks too highly of herself and her meek sidekick and a rare straight boy at the camp who lacks self-esteem.\" or \"Also, all the characters knew of the dangerous gingerbread man in the bakery but they never left to get help, it was as easy as opening the door and leaving, but no...can't do that.\". The algorithms detects words like 'their', 'all', 'its' which have tags adjectives.\n",
    "\n",
    "- The second mistake is it has not detected 'not' which might be a flaw in the dependency parser.\n",
    "\n",
    "\"I think the director needs prozac to direct this movie, the whole plot is so mess up, junior high student can write better story than this..what was he thinking. don't waste your money buying this movi\"\n",
    "\n",
    "    Opinion of 'plot': 'whole'\n",
    "\n",
    "- The algorithm should have detected \"so mess up\", so it might be a deficiency in the dependency parser as one of the previous examples.\n",
    "\n",
    "\"The similarities of these 2 movies are so prominent that it would actually be a waste of time and money to buy both - same characters, same plot only in a different setting.\"\n",
    "\n",
    "    Opinion of 'characters': 'both - same'\n",
    "\n",
    "- The algorithm should not have detected \"both\", although it's 'advmod' to \"same\".\n",
    "\n",
    "\n",
    "\"It's characters are re-created after Fellini's cartoons, or stereotypes (some based on real characters in Rimini), some comlpetely imaginary, all most funny and entertaining.\"\n",
    "\n",
    "- In this case 'imaginary', 'funny' and 'entertaining' haven't been detected, because of a flaw in the dependency parser: imaginary is head of sentence after the brackets, so the algorithm can't see it.\n",
    "\n",
    "\"There is no humor, no likable characters, not even a decent quote or one liner.   by the time it ambles toward its pathetic excuse for a conclusion\"\n",
    "\n",
    "    Opinion of 'characters': 'likable'\n",
    "\n",
    "- The algorithm is not searching for a determinant \"no\".\n",
    "\n",
    "#### Another problem with negation is in this sentence:\n",
    "\n",
    "\"The big problem with RH is that neither Ford or Benning are sympathetic characters.\"\n",
    "\n",
    "- The algorithm has only detected 'sympathetic' because it's not searching for 'preconj' which is 'neither'.\n",
    "\n",
    "The opinion extractor does not search for adverbs or adjectives recursively which creates problems in cases where we have more than one/two of them:\n",
    "\n",
    "\"There are many good supporting characters such as: Bridget Fonda, Woody Harrelson, Barnard Hughes and Julie Warner\"\n",
    "\n",
    "    Opinion of 'characters': 'many', 'good'\n",
    "\n",
    "- The algorithm has not detected the third 'amod'.\n",
    "\n",
    "\"The characters are very well developed and outlandish from the eccentricly outgoing berry(Jack Black) to the quiet weird geek type Dick.\"\n",
    "\n",
    "    Opinion of 'characters': 'well-developed', 'outlandish'\n",
    "\n",
    "- The algorithm has not found second adverb 'very'.\n",
    "\n",
    "#### Another example I'll give is for cases where there is a whole phrase describing an aspect word, and the algorithm fails to detect it:\n",
    "\n",
    "\"Depth in its story, depth in its characters, and this is why people of all age groups (I myself am 22+) will love this show.\"\n",
    "\n",
    "    Opinion of 'characters': 'its' (instead of 'depth in' for instance)\n",
    "    \n",
    "\"I enjoyed this movie because the plot was very different and not run of the mill like most horror movies.\"\n",
    "\n",
    "    Opinion of 'plot': 'very-different'\n",
    "\n",
    "    \n",
    "#### A big part of the failure of the opinion extractor on the sentences in the sample is the fact that the parser is not working as expected.\n",
    "\n",
    "\"Very good plot and detail\"\n",
    "\n",
    "    Opinion of 'plot': 'good'\n",
    "    \n",
    "- Here 'very' hasn't been detected because while it is supposed to be an advmod of very, it is actually another child of plot.\n",
    "\n",
    "\"It was not the actual story/plot, or it's accuracy that was important to me.\"\n",
    "\n",
    "    Opinion of 'plot': 'actual'\n",
    "    \n",
    "- Here 'not' is supposed to be child of plot (we have been told of this discrepancy in spaCy) but it is parsed as child of 'was'.\n",
    "\n",
    "\n",
    "#### As it can be seen from the counters of extensions, negation has not been counted at all and I'm assuming it is because of this particular descrepancy in the parser and the actual tree. Moreover, there weren't many sentences with classic negation in the sample.\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After looking at the results, it becomes obvious that the 'opinion extractor' can be improved in several ways. Firstly, recursions can be added for each of the extensions. By doing that, we'll make sure it finds all of the opinion bearing words for each extension, and within the extensions as well. Secondly, restrictions regarding tags of words can be more specific. In the places of the algorithm where it is searching for 'amod's that are adjectives, it can be developed to search for adjectives and verbs as well (and any other part of speech that is suitable). Thirdly, searching for negation can be developped a lot more. It can search for words like 'neither', 'nor' and 'no' (that have different dependencies - investigation into this is required.) Also, the algorithm can be made to ignore words like 'their', 'all', 'its' by checking for them in a list with words to be ignored. That way, even if the words have the right dependencies and tags, they won't be taken into consideration.\n",
    "Lastly, the algorithm can be complicated as to search for whole phrases that describe the aspect words. In many of the sentences with 'characters' we fail to establish the opinion of the author because it lies in the sentence as a whole, and not only in a couple of adjectives. By adding functionality, we'll get closer to the right opinion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
